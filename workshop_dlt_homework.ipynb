{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework**: Open source data ingestion for RAGs with dlt\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=qUNyfR_X2Mo\n",
    "\n",
    "In this hands-on workshop, we’ll learn how to build a data ingestion pipeline using dlt to load data from a REST API into LanceDB so you can have an always up to date RAG.\n",
    "\n",
    "​We’ll cover the following steps:\n",
    "\n",
    "* Extract data from REST APIs\n",
    "* Loading and vectorizing into LanceDB, which unlike other vector DBs stores the data _and_ the embeddings\n",
    "* Incremental loading\n",
    "\n",
    "​By the end of this workshop, you’ll be able to write a portable, OSS data pipeline for your RAG that you can deploy anywhere, such as python notebooks, virtual machines, or orchestrators like Airflow, Dagster or Mage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* Slides: [dlt-LLM-Zoomcamp.pdf](https://github.com/user-attachments/files/16131729/dlt.LLM.Zoomcamp.pdf)\n",
    "* [Google Colab notebook](https://colab.research.google.com/drive/1nNOybHdWQiwUUuJFZu__xvJxL_ADU3xl?usp=sharing) - make a copy to follow along!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "In the workshop, we extracted contents from two pages in notion titled \"Workshop: Benefits and Perks\" and \"Workshop: Working hours, PTO, and Vacation\". \n",
    "\n",
    "Repeat the same process for a third page titled \"Homework: Employee handbook\" (hidden from public view, but accessible via API key):\n",
    "\n",
    "1. Modify the REST API source to extract only this page.\n",
    "2. Write the output into a separate table called \"homework\".\n",
    "3. Remember to update the table name in all cells where you connect to a lancedb table.\n",
    "\n",
    "To do this you can use the [workshop Colab](https://colab.research.google.com/drive/1nNOybHdWQiwUUuJFZu__xvJxL_ADU3xl?usp=sharing) as a basis.\n",
    "\n",
    "Now, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Rows in LanceDB\n",
    "\n",
    "How many rows does the lancedb table \"notion_pages__homework\" have?\n",
    "\n",
    "* 14\n",
    "* 15\n",
    "* 16\n",
    "* 17\n",
    "\n",
    "### Notes:\n",
    "\n",
    ":white_check_mark: 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 4. Write the pipeline code\n",
    "\n",
    "**Note**: We first go over the code step by step before putting it into runnable cells\n",
    "\n",
    "1. Import necessary modules (run this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from rest_api import RESTAPIConfig, rest_api_source\n",
    "\n",
    "from dlt.sources.helpers.rest_client.paginators import BasePaginator, JSONResponsePaginator\n",
    "from dlt.sources.helpers.requests import Response, Request\n",
    "\n",
    "from dlt.destinations.adapters import lancedb_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Configure the dlt rest api source to connect to and extract the relevant data out from the Notion REST API.\n",
    "\n",
    "    Our notion space has multiple pages ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Note on pagination:\n",
    "\n",
    "  Different REST APIs might use different strategies to handle paginated responses. dlt has built-in support for [most common pagination mechanisms](https://dlthub.com/docs/general-usage/http/rest-client#paginators), and these can be explicity passed inside the configuration like shown above.\n",
    "\n",
    "  However in most cases, it won't be necessary to explicity specify the pagination strategy, since dlt detects it automatically.\n",
    "\n",
    "  In case the specific pagination is not supported by dlt yet, then you can also implement a custom paginator. For example, dlt does not have a built-in paginator for POST methods, so we write our own paginator. We take the [code provided in the docs for it](https://dlthub.com/docs/general-usage/http/rest-client#example-2-creating-a-paginator-for-post-requests), and make small modifications to it based on the [notion API documentation](https://developers.notion.com/reference/intro#parameters-for-paginated-requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostBodyPaginator(BasePaginator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cursor = None\n",
    "\n",
    "    def update_state(self, response: Response) -> None:\n",
    "        # Assuming the API returns an empty list when no more data is available\n",
    "        if not response.json():\n",
    "            self._has_next_page = False\n",
    "        else:\n",
    "            self.cursor = response.json().get(\"next_cursor\")\n",
    "            if self.cursor is None:\n",
    "                self._has_next_page = False\n",
    "\n",
    "    def update_request(self, request: Request) -> None:\n",
    "        if request.json is None:\n",
    "            request.json = {}\n",
    "\n",
    "        # Add the cursor to the request body\n",
    "        request.json[\"start_cursor\"] = self.cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract relevant content from the response body\n",
    "\n",
    "  The response returned from the API is a nested JSON which we need to pre-process before using it anywhere. dlt can unnest json automatically, but since the Notion API is a little tricky it's better to pre-process this first so we have a more structured DB as a result.\n",
    "  \n",
    "  One way to do this is to pass the JSON response through a transformation function that will extract only the relevant data from the JSON body (we later add this as a mapping to the resource):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added imports\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def extract_page_content(response):\n",
    "    block_id = response[\"id\"]\n",
    "    last_edited_time = response[\"last_edited_time\"]\n",
    "    block_type = response.get(\"type\", \"Not paragraph\")\n",
    "    if block_type != \"paragraph\":\n",
    "        content = \"\"\n",
    "    else:\n",
    "        try:\n",
    "            content = response[\"paragraph\"][\"rich_text\"][0][\"plain_text\"]\n",
    "        except IndexError:\n",
    "            content = \"\"\n",
    "    return {\n",
    "        \"block_id\": block_id,\n",
    "        \"block_type\": block_type,\n",
    "        \"content\": content,\n",
    "        \"last_edited_time\": last_edited_time,\n",
    "        \"inserted_at_time\": datetime.now(timezone.utc)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run the pipeline\n",
    "\n",
    "Run this block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Projects\\rye-demo\\.venv\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-05T23:33:00.000Z\n",
      "Pipeline company_policies load step completed in ---\n",
      "0 load package(s) were loaded to destination LanceDB and into dataset None\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x000002187CAE0770> location to store data\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime, timezone\n",
    "\n",
    "# class PostBodyPaginator(BasePaginator):\n",
    "#     ...\n",
    "\n",
    "\n",
    "# @dlt.resource(name=\"employee_handbook\")\n",
    "@dlt.resource(name=\"homework\")\n",
    "def rest_api_notion_resource():\n",
    "    notion_config: RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\n",
    "                \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "            },\n",
    "            \"headers\":{\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Notion-Version\": \"2022-06-28\"\n",
    "            }\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        # \"query\": \"workshop\",\n",
    "                        \"query\": \"homework\",\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"data_selector\": \"results\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # yield from rest_api_source(notion_config,name=\"employee_handbook\")\n",
    "    yield from rest_api_source(notion_config,name=\"homework\")\n",
    "\n",
    "# def extract_page_content(response):\n",
    "#     ...\n",
    "\n",
    "\n",
    "@dlt.resource(\n",
    "    # name=\"employee_handbook\",\n",
    "    name=\"homework\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"block_id\",\n",
    "    columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}\n",
    "    )\n",
    "def rest_api_notion_incremental(\n",
    "    last_edited_time = dlt.sources.incremental(\"last_edited_time\", initial_value=\"2024-06-26T08:16:00.000Z\",primary_key=(\"block_id\"))\n",
    "):\n",
    "    last_value = last_edited_time.last_value\n",
    "    print(last_value)\n",
    "\n",
    "    for block in rest_api_notion_resource.add_map(extract_page_content):\n",
    "        if not(len(block[\"content\"])):\n",
    "            continue\n",
    "        yield block\n",
    "\n",
    "def load_notion() -> None:\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"company_policies\",\n",
    "        destination=\"lancedb\",\n",
    "        dataset_name=\"notion_pages\",\n",
    "        # full_refresh=True\n",
    "    )\n",
    "\n",
    "    load_info = pipeline.run(\n",
    "        lancedb_adapter(\n",
    "            rest_api_notion_incremental,\n",
    "            embed=\"content\"\n",
    "        ),\n",
    "        # table_name=\"employee_handbook\",\n",
    "        table_name=\"homework\",\n",
    "        write_disposition=\"merge\"\n",
    "    )\n",
    "    print(load_info)\n",
    "\n",
    "load_notion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homework.shape=(17, 9)\n",
      "Q1 Answer: The lancedb table \"notion_pages__homework\" have 17 rows\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "\n",
    "\n",
    "db = lancedb.connect(\".lancedb\")\n",
    "dbtable = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "homework = dbtable.to_pandas()\n",
    "print(f\"{homework.shape=}\")\n",
    "print('Q1 Answer: The lancedb table \"notion_pages__homework\" have {} rows'.format(homework.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Running the Pipeline: Last edited time\n",
    "\n",
    "In the demo, we created an incremental dlt resource `rest_api_notion_incremental` that keeps track of `last_edited_time`. What value does it store after you've run your pipeline once? (Hint: you will be able to get this value by performing some aggregation function on the column `last_edited_time` of the table)\n",
    "\n",
    "* `Timestamp('2024-07-05 22:34:00+0000', tz='UTC') (OR \"2024-07-05T22:34:00.000Z\")`\n",
    "* `Timestamp('2024-07-05 23:33:00+0000', tz='UTC') (OR \"2024-07-05T23:33:00.000Z\")`\n",
    "* `Timestamp('2024-07-05 23:52:00+0000', tz='UTC') (OR \"2024-07-05T23:52:00.000Z\")`\n",
    "* `Timestamp('2024-07-05 22:56:00+0000', tz='UTC') (OR \"2024-07-05T22:56:00.000Z\")`\n",
    "\n",
    "### Notes:\n",
    "\n",
    ":white_check_mark: `Timestamp('2024-07-05 23:33:00+0000', tz='UTC') (OR \"2024-07-05T23:33:00.000Z\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2024-07-05 22:34:00+00:00\n",
      "1    2024-07-05 22:38:00+00:00\n",
      "2    2024-07-05 22:52:00+00:00\n",
      "3    2024-07-05 22:52:00+00:00\n",
      "4    2024-07-05 22:52:00+00:00\n",
      "5    2024-07-05 22:52:00+00:00\n",
      "6    2024-07-05 22:52:00+00:00\n",
      "7    2024-07-05 22:52:00+00:00\n",
      "8    2024-07-05 23:33:00+00:00\n",
      "9    2024-07-05 22:52:00+00:00\n",
      "10   2024-07-05 22:52:00+00:00\n",
      "11   2024-07-05 22:52:00+00:00\n",
      "12   2024-07-05 22:52:00+00:00\n",
      "13   2024-07-05 22:52:00+00:00\n",
      "14   2024-07-05 22:52:00+00:00\n",
      "15   2024-07-05 22:54:00+00:00\n",
      "16   2024-07-05 22:56:00+00:00\n",
      "Name: last_edited_time, dtype: datetime64[us, UTC]\n",
      "Q2 Answer: last_edited_time=Timestamp('2024-07-05 23:33:00+0000', tz='UTC')\n"
     ]
    }
   ],
   "source": [
    "print(homework[\"last_edited_time\"])\n",
    "\n",
    "last_edited_time = homework[\"last_edited_time\"].max()\n",
    "print(f\"Q2 Answer: {last_edited_time=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Ask the Assistant \n",
    "\n",
    "Find out with the help of the AI assistant: how many PTO days are the employees entitled to in a year?  \n",
    "\n",
    "* 20\n",
    "* 25\n",
    "* 30\n",
    "* 35\n",
    "\n",
    "### Notes:\n",
    "\n",
    ":white_check_mark: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a function that can retrieve content from lancedb relevant to the user query\n",
    "  \n",
    "  With LanceDB, you don't have to explicity embed the question. LanceDB stores information on the embedding model used and automatically embeds the question.\n",
    "\n",
    "  We use the `db_table.search()` function to query the DB and then limit it to the top 2 most similar results and return that as the context to pass to the RAG.\n",
    "\n",
    "  Limiting results is important because otherwise there might be too much confusing information. Similarly only picking the top choice might not give enough information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_from_lancedb(dbtable, question, top_k=2):\n",
    "\n",
    "    query_results = dbtable.search(query=question).to_list()\n",
    "    context = \"\\n\".join([result[\"content\"] for result in query_results[:top_k]])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Finally we define a very basic RAG. We define a simple system prompt, retrieve the relevant context for the user query with the function defined above and then send the user question and the context to the `llama2-uncensored` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added imports\n",
    "import ollama\n",
    "\n",
    "\n",
    "def main():\n",
    "  # Connect to the lancedb table\n",
    "  db = lancedb.connect(\".lancedb\")\n",
    "#   dbtable = db.open_table(\"notion_pages___employee_handbook\")\n",
    "  dbtable = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "  # A system prompt telling ollama to accept input in the form of \"Question: ... ; Context: ...\"\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps users understand policies inside a company's employee handbook. The user will first ask you a question and then provide you relevant paragraphs from the handbook as context. Please answer the question based on the provided context. For any details missing in the paragraph, encourage the employee to contact the HR for that information. Please keep the responses conversational.\"}\n",
    "  ]\n",
    "\n",
    "  while True:\n",
    "    # Accept user question\n",
    "    question = input(\"You: \")\n",
    "\n",
    "    # Retrieve the relevant paragraphs on the question\n",
    "    context = retrieve_context_from_lancedb(dbtable,question,top_k=2)\n",
    "\n",
    "    # Create a user prompt using the question and retrieved context\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": f\"Question: '{question}'; Context:'{context}'\"}\n",
    "    )\n",
    "\n",
    "    # Get the response from the LLM\n",
    "    response = ollama.chat(\n",
    "        # model=\"llama2-uncensored\",\n",
    "        model=\"gemma:2b\",\n",
    "        messages=messages\n",
    "    )\n",
    "    response_content = response['message']['content']\n",
    "    print(f\"Assistant: {response_content}\")\n",
    "\n",
    "    # Add the response into the context window\n",
    "    messages.append(\n",
    "        {\"role\": \"assistant\", \"content\":response_content}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the RAG! Some example questions you can ask:\n",
    "\n",
    "* How many vacation days do I get?\n",
    "* Can I get maternity leave?\n",
    "\n",
    "**Note**: This is a very basic implementation of a RAG, since this workshop is mainly about data ingestion. So expect some weird answers. If you do stop and restart the cell, you will need to rerun the cell containing `ollama serve` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Okay, I understand that employees receive 30 days of PTO per year. However, the context does not specify how many of these 30 days can be taken at any given time during the year. Could you please provide more information about this?\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
