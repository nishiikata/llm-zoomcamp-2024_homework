{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Homework\n",
    "\n",
    "## Homework: Evaluation and Monitoring\n",
    "\n",
    "In this homework, we'll evaluate the quality of our RAG system.\n",
    "\n",
    "> It's possible that your answers won't match exactly. If it's the case, select the closest one.\n",
    "\n",
    "Solution:\n",
    "\n",
    "* Video: TBA\n",
    "* Notebook: TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "Let's start by getting the dataset. We will use the data we generated in the module.\n",
    "\n",
    "In particular, we'll evaluate the quality of our RAG system\n",
    "with [gpt-4o-mini](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv)\n",
    "\n",
    "\n",
    "Read it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas and defining `github_url` variable\n",
    "import pandas as pd\n",
    "\n",
    "github_url: str = \"https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use only the first 300 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Getting the embeddings model\n",
    "\n",
    "Now, get the embeddings model `multi-qa-mpnet-base-dot-v1` from\n",
    "[the Sentence Transformer library](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview)\n",
    "\n",
    "> Note: this is not the same model as in HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added code block defining `model_name` variable\n",
    "model_name: str = \"multi-qa-mpnet-base-dot-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\conda-3.12\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings for the first LLM answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the first value of the resulting vector?\n",
    "\n",
    "* -0.42\n",
    "* -0.22\n",
    "* -0.02\n",
    "* 0.21\n",
    "\n",
    "### Notes:\n",
    "\n",
    ":white_check_mark: -0.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Answer: The first value of the resulting vector is -0.42\n"
     ]
    }
   ],
   "source": [
    "type Embedding = \"list[Tensor] | ndarray | Tensor\"\n",
    "v: Embedding = embedding_model.encode(answer_llm)\n",
    "print(\"Q1 Answer: The first value of the resulting vector is {:.2f}\".format(v[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Computing the dot product\n",
    "\n",
    "\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the `evaluations` list\n",
    "\n",
    "What's the 75% percentile of the score?\n",
    "\n",
    "* 21.67\n",
    "* 31.67\n",
    "* 41.67\n",
    "* 51.67\n",
    "\n",
    "### Notes:\n",
    "\n",
    ":white_check_mark: 31.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class AnswerRecord(TypedDict):\n",
    "    answer_orig: str\n",
    "    answer_llm: str\n",
    "\n",
    "\n",
    "def compute_similarity(\n",
    "    record: AnswerRecord,\n",
    "    model: \"SentenceTransformer\"\n",
    "    ) -> \"Embedding\":\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = model.encode(answer_llm)\n",
    "    v_orig = model.encode(answer_orig)\n",
    "    \n",
    "    return v_llm.dot(v_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_records: list[dict] = df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ca2e243a4e463d9f28da83b4a2ac54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "evaluations: list[float] = []\n",
    "for record in tqdm(df_records):\n",
    "    evaluations.append(compute_similarity(record, embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    300.000000\n",
      "mean      27.495996\n",
      "std        6.384745\n",
      "min        4.547922\n",
      "25%       24.307833\n",
      "50%       28.336869\n",
      "75%       31.674322\n",
      "max       39.476021\n",
      "dtype: float64\n",
      "Q2 Answer: The 75% percentile of the score is 31.67\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "\n",
    "print(pd.Series(evaluations).describe())\n",
    "\n",
    "quantiles: list[float] = statistics.quantiles(evaluations, n=4, method=\"inclusive\")\n",
    "print(\"Q2 Answer: The 75% percentile of the score is {:.2f}\".format(quantiles[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Computing the cosine\n",
    "\n",
    "From Q2, we can see that the results are not within the [0, 1] range. It's because the vectors coming from this model are not normalized.\n",
    "\n",
    "So we need to normalize them.\n",
    "\n",
    "To do it, we \n",
    "\n",
    "* Compute the norm of a vector\n",
    "* Divide each element by this norm\n",
    "\n",
    "So, for vector `v`, it'll be `v / ||v||`\n",
    "\n",
    "In numpy, this is how you do it:\n",
    "\n",
    "```python\n",
    "norm = np.sqrt((v * v).sum())\n",
    "v_norm = v / norm\n",
    "```\n",
    "\n",
    "Let's put it into a function and then compute dot product \n",
    "between normalized vectors. This will give us cosine similarity\n",
    "\n",
    "What's the 75% cosine in the scores?\n",
    "\n",
    "* 0.63\n",
    "* 0.73\n",
    "* 0.83\n",
    "* 0.93\n",
    "\n",
    "### Notes:\n",
    "\n",
    ":white_check_mark: 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AnswerRecord(TypedDict):\n",
    "    answer_orig: str\n",
    "    answer_llm: str\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(\n",
    "    record: AnswerRecord,\n",
    "    model: \"SentenceTransformer\"\n",
    "    ) -> \"Embedding\":\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = model.encode(answer_llm)\n",
    "    v_orig = model.encode(answer_orig)\n",
    "\n",
    "    v_llm_norm = v_llm / np.sqrt((v_llm ** 2).sum())\n",
    "    v_orig_norm = v_orig / np.sqrt((v_orig ** 2).sum())\n",
    "    \n",
    "    return v_llm_norm.dot(v_orig_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b722439617f04dff9db6197950957fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "evaluations_cosine: list[float] = []\n",
    "for record in tqdm(df_records):\n",
    "    evaluations_cosine.append(\n",
    "        compute_cosine_similarity(record, embedding_model)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    300.000000\n",
      "mean       0.728392\n",
      "std        0.157755\n",
      "min        0.125357\n",
      "25%        0.651274\n",
      "50%        0.763761\n",
      "75%        0.836235\n",
      "max        0.958796\n",
      "dtype: float64\n",
      "Q3 Answer: The 75% cosine in the scores is 0.8362\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "\n",
    "print(pd.Series(evaluations_cosine).describe())\n",
    "\n",
    "quantiles_cosine: list[float] = statistics.quantiles(evaluations_cosine, n=4, method=\"inclusive\")\n",
    "print(\"Q3 Answer: The 75% cosine in the scores is {:.4f}\".format(quantiles_cosine[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Rouge\n",
    "\n",
    "Now we will explore an alternative metric - the ROUGE score.  \n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it:\n",
    "\n",
    "```bash\n",
    "pip install rouge\n",
    "```\n",
    "\n",
    "(The latest version at the moment of writing is `1.0.1`)\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (`doc_id=5170565b`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_llm     Yes, all sessions are recorded, so if you miss...\n",
       "answer_orig    Everything is recorded, so you won’t miss anyt...\n",
       "document                                                5170565b\n",
       "question                    Are sessions recorded if I miss one?\n",
       "course                                 machine-learning-zoomcamp\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining `r` variable\n",
    "r = df.iloc[10]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three scores: `rouge-1`, `rouge-2` and `rouge-l`, and precision, recall and F1 score for each.\n",
    "\n",
    "* `rouge-1` - the overlap of unigrams,\n",
    "* `rouge-2` - bigrams,\n",
    "* `rouge-l` - the longest common subsequence\n",
    "\n",
    "What's the F score for `rouge-1`?\n",
    "\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55\n",
    "- 0.65\n",
    "\n",
    "### Notes:\n",
    "\n",
    ":white_check_mark: 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.45454545454545453, 'p': 0.45454545454545453, 'f': 0.45454544954545456}, 'rouge-2': {'r': 0.21621621621621623, 'p': 0.21621621621621623, 'f': 0.21621621121621637}, 'rouge-l': {'r': 0.3939393939393939, 'p': 0.3939393939393939, 'f': 0.393939388939394}}\n",
      "Q4 Answer: The F score for `rouge-1` is 0.45\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "\n",
    "print(\"Q4 Answer: The F score for `rouge-1` is {:.2f}\".format(scores[\"rouge-1\"][\"f\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Average rouge score\n",
    "\n",
    "Let's compute the average between `rouge-1`, `rouge-2` and `rouge-l` for the same record from Q4\n",
    "\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55\n",
    "- 0.65\n",
    "\n",
    "### Notes:\n",
    "\n",
    ":white_check_mark: 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Average rouge score for all the data points\n",
    "\n",
    "Now let's compute the score for all the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1 = scores['rouge-1']['f']\n",
    "rouge_2 = scores['rouge-2']['f']\n",
    "rouge_l = scores['rouge-l']['f']\n",
    "rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 Answer: The average between `rouge-1`, `rouge-2` and `rouge-l` for the same record from Q4 is 0.35\n"
     ]
    }
   ],
   "source": [
    "print(\"Q5 Answer: The average between `rouge-1`, `rouge-2` \"\n",
    "      + \"and `rouge-l` for the same record from Q4 is {:.2f}\"\n",
    "      .format(rouge_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a dataframe from them\n",
    "\n",
    "What's the agerage `rouge_2` across all the records?\n",
    "\n",
    "- 0.10\n",
    "- 0.20\n",
    "- 0.30\n",
    "- 0.40\n",
    "\n",
    "### Notes:\n",
    "\n",
    ":white_check_mark: 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f1f1e784824dbfbccf96420dc697ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.061224489795918366,\n",
       "   'p': 0.21428571428571427,\n",
       "   'f': 0.09523809178130524},\n",
       "  'rouge-2': {'r': 0.017543859649122806,\n",
       "   'p': 0.07142857142857142,\n",
       "   'f': 0.028169010918468917},\n",
       "  'rouge-l': {'r': 0.061224489795918366,\n",
       "   'p': 0.21428571428571427,\n",
       "   'f': 0.09523809178130524}},\n",
       " {'rouge-1': {'r': 0.08163265306122448,\n",
       "   'p': 0.26666666666666666,\n",
       "   'f': 0.12499999641113292},\n",
       "  'rouge-2': {'r': 0.03508771929824561,\n",
       "   'p': 0.13333333333333333,\n",
       "   'f': 0.05555555225694465},\n",
       "  'rouge-l': {'r': 0.061224489795918366, 'p': 0.2, 'f': 0.09374999641113295}},\n",
       " {'rouge-1': {'r': 0.32653061224489793,\n",
       "   'p': 0.5714285714285714,\n",
       "   'f': 0.41558441095631643},\n",
       "  'rouge-2': {'r': 0.14035087719298245,\n",
       "   'p': 0.24242424242424243,\n",
       "   'f': 0.17777777313333343},\n",
       "  'rouge-l': {'r': 0.30612244897959184,\n",
       "   'p': 0.5357142857142857,\n",
       "   'f': 0.3896103849822905}},\n",
       " {'rouge-1': {'r': 0.16326530612244897, 'p': 0.32, 'f': 0.2162162117421476},\n",
       "  'rouge-2': {'r': 0.03508771929824561,\n",
       "   'p': 0.07142857142857142,\n",
       "   'f': 0.047058819111419105},\n",
       "  'rouge-l': {'r': 0.14285714285714285, 'p': 0.28, 'f': 0.18918918471512064}}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_rouge_scores = []\n",
    "for record in tqdm(df_records):\n",
    "    rogue_scores = rouge_scorer.get_scores(record['answer_llm'], record['answer_orig'])[0]\n",
    "    record_rouge_scores.append(rogue_scores)\n",
    "\n",
    "record_rouge_scores[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-2': {'r': 0.19861258009846802, 'p': 0.2586264651699855, 'f': 0.20696501983423318}}\n",
      "Q6 Answer: The average score for `rouge-2` across all the records is 0.2070\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class RougeDict(TypedDict):\n",
    "    r: float\n",
    "    p: float\n",
    "    f: float\n",
    "\n",
    "\n",
    "Rouge2Key = TypedDict(\"Rouge2\", { \"rouge-2\": RougeDict })\n",
    "\n",
    "\n",
    "def add_rouge_2(rouge: Rouge2Key, rouge2: Rouge2Key) -> Rouge2Key:\n",
    "    return {\n",
    "        \"rouge-2\": {\n",
    "            \"r\": rouge[\"rouge-2\"][\"r\"] + rouge2[\"rouge-2\"][\"r\"],\n",
    "            \"p\": rouge[\"rouge-2\"][\"p\"] + rouge2[\"rouge-2\"][\"p\"],\n",
    "            \"f\": rouge[\"rouge-2\"][\"f\"] + rouge2[\"rouge-2\"][\"f\"],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "       \n",
    "sum_rouge_2: Rouge2Key = reduce(add_rouge_2, record_rouge_scores)\n",
    "\n",
    "rouge_2_dict: RougeDict = sum_rouge_2[\"rouge-2\"]\n",
    "for key in rouge_2_dict:\n",
    "    rouge_2_dict[key] = rouge_2_dict[key] / len(record_rouge_scores)\n",
    "\n",
    "print(sum_rouge_2)\n",
    "print(\"Q6 Answer: The average score for `rouge-2` across all the records is {:.4f}\".format(rouge_2_dict[\"f\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
